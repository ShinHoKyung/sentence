{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import argparse\n",
    "import copy\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath,label):\n",
    "    data = []\n",
    "    with codecs.open(fpath,'r','utf-8',errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            data.append((l.split(' '),label))\n",
    "    return data\n",
    "pos = load_data('./dataset/rt-polarity.pos',1)\n",
    "neg = load_data('./dataset/rt-polarity.neg',0)\n",
    "data = pos+neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = max([len(sentence) for sentence, _ in data])\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for d, _ in data:\n",
    "    for w in d:\n",
    "        if w not in vocab:\n",
    "            vocab.append(w)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "i2w = {i:w for i,w in enumerate(vocab)}\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "wv_matrix = []\n",
    "for i in range(len(vocab)):\n",
    "            word = i2w[i]\n",
    "            if word in word_vectors.vocab:\n",
    "                wv_matrix.append(word_vectors.word_vec(word))\n",
    "            else:\n",
    "                wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "\n",
    "wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "wv_matrix.append(np.zeros(300).astype(\"float32\"))\n",
    "wv_matrix = np.array(wv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_idx = (int)(len(data)*0.8)\n",
    "random.shuffle(data)\n",
    "train_data = data[:div_idx]\n",
    "test_data = data[div_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,vocab_size,embd_size,out_chs,filter_heights,pretrained_vec):\n",
    "        super(Net,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+2,embd_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_vec))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1,out_chs,(fh,embd_size)) for fh in filter_heights])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filter_heights),1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "        x = [F.max_pool1d(i,i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        probs = F.sigmoid(x)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter :  [3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0,loss : 116.459\n",
      "epoch : 1,loss : 111.562\n",
      "epoch : 2,loss : 104.062\n",
      "epoch : 3,loss : 95.606\n",
      "epoch : 4,loss : 88.790\n",
      "epoch : 5,loss : 83.161\n",
      "epoch : 6,loss : 79.748\n",
      "epoch : 7,loss : 75.888\n",
      "epoch : 8,loss : 72.786\n",
      "epoch : 9,loss : 70.050\n",
      "epoch : 10,loss : 67.809\n",
      "epoch : 11,loss : 64.930\n",
      "epoch : 12,loss : 62.838\n",
      "epoch : 13,loss : 60.718\n",
      "epoch : 14,loss : 57.756\n",
      "epoch : 15,loss : 56.360\n",
      "epoch : 16,loss : 53.740\n",
      "epoch : 17,loss : 51.481\n",
      "epoch : 18,loss : 49.253\n",
      "epoch : 19,loss : 46.844\n",
      "epoch : 20,loss : 44.070\n",
      "epoch : 21,loss : 41.968\n",
      "epoch : 22,loss : 39.649\n",
      "epoch : 23,loss : 37.803\n",
      "epoch : 24,loss : 36.137\n",
      "epoch : 25,loss : 33.495\n",
      "epoch : 26,loss : 32.319\n",
      "epoch : 27,loss : 30.072\n",
      "epoch : 28,loss : 29.044\n",
      "epoch : 29,loss : 27.127\n",
      "epoch : 30,loss : 25.298\n",
      "epoch : 31,loss : 24.396\n",
      "epoch : 32,loss : 22.690\n",
      "epoch : 33,loss : 21.419\n",
      "epoch : 34,loss : 20.130\n",
      "epoch : 35,loss : 18.996\n",
      "epoch : 36,loss : 17.532\n",
      "epoch : 37,loss : 17.212\n",
      "epoch : 38,loss : 15.326\n",
      "epoch : 39,loss : 14.377\n",
      "epoch : 40,loss : 13.965\n",
      "epoch : 41,loss : 13.001\n",
      "epoch : 42,loss : 12.292\n",
      "epoch : 43,loss : 11.660\n",
      "epoch : 44,loss : 10.928\n",
      "epoch : 45,loss : 10.257\n",
      "epoch : 46,loss : 9.598\n",
      "epoch : 47,loss : 9.096\n",
      "epoch : 48,loss : 8.303\n",
      "epoch : 49,loss : 8.634\n",
      "epoch : 50,loss : 7.663\n",
      "epoch : 51,loss : 7.374\n",
      "epoch : 52,loss : 7.055\n",
      "epoch : 53,loss : 6.929\n",
      "epoch : 54,loss : 6.878\n",
      "epoch : 55,loss : 6.208\n",
      "epoch : 56,loss : 6.012\n",
      "epoch : 57,loss : 5.940\n",
      "epoch : 58,loss : 5.367\n",
      "epoch : 59,loss : 5.475\n",
      "epoch : 60,loss : 5.306\n",
      "epoch : 61,loss : 4.929\n",
      "epoch : 62,loss : 4.847\n",
      "epoch : 63,loss : 4.222\n",
      "epoch : 64,loss : 4.496\n",
      "epoch : 65,loss : 4.257\n",
      "epoch : 66,loss : 4.090\n",
      "epoch : 67,loss : 4.025\n",
      "epoch : 68,loss : 3.977\n",
      "epoch : 69,loss : 3.650\n",
      "epoch : 70,loss : 3.652\n",
      "epoch : 71,loss : 3.587\n",
      "epoch : 72,loss : 3.570\n",
      "epoch : 73,loss : 3.341\n",
      "epoch : 74,loss : 3.337\n",
      "epoch : 75,loss : 3.101\n",
      "epoch : 76,loss : 2.900\n",
      "epoch : 77,loss : 3.081\n",
      "epoch : 78,loss : 2.850\n",
      "epoch : 79,loss : 2.730\n",
      "epoch : 80,loss : 2.611\n",
      "epoch : 81,loss : 2.755\n",
      "epoch : 82,loss : 2.711\n",
      "epoch : 83,loss : 2.661\n",
      "epoch : 84,loss : 2.466\n",
      "epoch : 85,loss : 2.476\n",
      "epoch : 86,loss : 2.526\n",
      "epoch : 87,loss : 2.332\n",
      "epoch : 88,loss : 2.368\n",
      "epoch : 89,loss : 2.384\n",
      "epoch : 90,loss : 2.229\n",
      "epoch : 91,loss : 2.039\n",
      "epoch : 92,loss : 2.145\n",
      "epoch : 93,loss : 1.970\n",
      "epoch : 94,loss : 2.014\n",
      "epoch : 95,loss : 2.136\n",
      "epoch : 96,loss : 1.813\n",
      "epoch : 97,loss : 2.060\n",
      "epoch : 98,loss : 1.872\n",
      "epoch : 99,loss : 1.849\n",
      "Training avg loss : 23.129\n",
      "Test acc : 0.784 (1673/2133)\n",
      "Test loss : 0.158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model,data,batch_size,n_epoch):\n",
    "    model.train()\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(),lr = 0.1)\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        random.shuffle(data)\n",
    "        for i in range(0,len(data)-batch_size,batch_size):\n",
    "            in_data,labels = [],[]\n",
    "            for sentence, label in data[i:i+batch_size]:\n",
    "                index_vec = [w2i[w] for w in sentence]\n",
    "                pad_len = max(0,max_sentence_len - len(index_vec))\n",
    "                index_vec +=[0]*pad_len\n",
    "                index_vec = index_vec[:max_sentence_len]\n",
    "                in_data.append(index_vec)\n",
    "                labels.append(label)\n",
    "            sent_var = Variable(torch.LongTensor(in_data))\n",
    "            if use_cuda:\n",
    "                sent_var = sent_var.cuda()\n",
    "            target_var = Variable(torch.Tensor(labels).unsqueeze(1))\n",
    "            if use_cuda:\n",
    "                target_var = target_var.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(sent_var)\n",
    "            loss = F.binary_cross_entropy(probs,target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data[0]\n",
    "        \n",
    "        print('epoch : {:d},loss : {:.3f}'.format(epoch,epoch_loss))\n",
    "        losses.append(epoch_loss)\n",
    "    print('Training avg loss : {:.3f}'.format(sum(losses)/len(losses)))\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "def test(model,data, n_test,min_sentence_len):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for sentence,label in data[:n_test]:\n",
    "        if len(sentence) < min_sentence_len:\n",
    "            continue\n",
    "        index_vec = [w2i[w] for w in sentence]\n",
    "        sent_var = Variable(torch.LongTensor([index_vec]))\n",
    "        if use_cuda:\n",
    "            sent_var = sent_var.cuda()\n",
    "        out = model(sent_var)\n",
    "        score = out.data[0][0]\n",
    "        pred = 1 if score > .5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        loss += math.pow((label-score),2)\n",
    "    print('Test acc : {:.3f} ({:d}/{:d})'.format(correct/n_test,correct,n_test))\n",
    "    print('Test loss : {:.3f}'.format(loss/n_test))\n",
    "    \n",
    "out_ch = 100\n",
    "embd_size = 300\n",
    "batch_size = 50\n",
    "n_epoch = 100\n",
    "filter_size = [3,4,5]\n",
    "print('filter : ',filter_size)\n",
    "model = Net(vocab_size,embd_size,out_ch,filter_size,wv_matrix)\n",
    "model,losses = train(model,train_data,batch_size,n_epoch)\n",
    "test(model,test_data,len(test_data),max(filter_size))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
