{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import argparse\n",
    "import copy\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath,label):\n",
    "    data = []\n",
    "    with codecs.open(fpath,'r','utf-8',errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            data.append((l.split(' '),label))\n",
    "    return data\n",
    "pos = load_data('./dataset/rt-polarity.pos',1)\n",
    "neg = load_data('./dataset/rt-polarity.neg',0)\n",
    "data = pos+neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = max([len(sentence) for sentence, _ in data])\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for d, _ in data:\n",
    "    for w in d:\n",
    "        if w not in vocab:\n",
    "            vocab.append(w)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "i2w = {i:w for i,w in enumerate(vocab)}\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "wv_matrix = []\n",
    "for i in range(len(vocab)):\n",
    "            word = i2w[i]\n",
    "            if word in word_vectors.vocab:\n",
    "                wv_matrix.append(word_vectors.word_vec(word))\n",
    "            else:\n",
    "                wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "\n",
    "wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "wv_matrix.append(np.zeros(300).astype(\"float32\"))\n",
    "wv_matrix = np.array(wv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_idx = (int)(len(data)*0.8)\n",
    "random.shuffle(data)\n",
    "train_data = data[:div_idx]\n",
    "test_data = data[div_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,vocab_size,embd_size,out_chs,filter_heights,pretrained_vec):\n",
    "        super(Net,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+2,embd_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_vec))\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1,out_chs,(fh,embd_size)) for fh in filter_heights])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filter_heights),1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "        x = [F.max_pool1d(i,i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        probs = F.sigmoid(x)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter :  [3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0,loss : 116.320\n",
      "epoch : 1,loss : 111.279\n",
      "epoch : 2,loss : 103.377\n",
      "epoch : 3,loss : 94.109\n",
      "epoch : 4,loss : 86.616\n",
      "epoch : 5,loss : 81.293\n",
      "epoch : 6,loss : 77.148\n",
      "epoch : 7,loss : 73.412\n",
      "epoch : 8,loss : 70.028\n",
      "epoch : 9,loss : 66.794\n",
      "epoch : 10,loss : 63.556\n",
      "epoch : 11,loss : 60.034\n",
      "epoch : 12,loss : 58.160\n",
      "epoch : 13,loss : 54.781\n",
      "epoch : 14,loss : 52.369\n",
      "epoch : 15,loss : 49.389\n",
      "epoch : 16,loss : 46.903\n",
      "epoch : 17,loss : 44.002\n",
      "epoch : 18,loss : 41.563\n",
      "epoch : 19,loss : 38.758\n",
      "epoch : 20,loss : 36.709\n",
      "epoch : 21,loss : 34.081\n",
      "epoch : 22,loss : 31.425\n",
      "epoch : 23,loss : 29.689\n",
      "epoch : 24,loss : 27.312\n",
      "epoch : 25,loss : 25.276\n",
      "epoch : 26,loss : 23.804\n",
      "epoch : 27,loss : 21.695\n",
      "epoch : 28,loss : 19.586\n",
      "epoch : 29,loss : 18.416\n",
      "epoch : 30,loss : 17.127\n",
      "epoch : 31,loss : 15.413\n",
      "epoch : 32,loss : 14.095\n",
      "epoch : 33,loss : 13.419\n",
      "epoch : 34,loss : 12.470\n",
      "epoch : 35,loss : 11.217\n",
      "epoch : 36,loss : 10.446\n",
      "epoch : 37,loss : 9.896\n",
      "epoch : 38,loss : 8.708\n",
      "epoch : 39,loss : 8.153\n",
      "epoch : 40,loss : 7.940\n",
      "epoch : 41,loss : 7.254\n",
      "epoch : 42,loss : 6.739\n",
      "epoch : 43,loss : 6.017\n",
      "epoch : 44,loss : 6.147\n",
      "epoch : 45,loss : 5.663\n",
      "epoch : 46,loss : 5.076\n",
      "epoch : 47,loss : 5.001\n",
      "epoch : 48,loss : 4.610\n",
      "epoch : 49,loss : 4.213\n",
      "epoch : 50,loss : 3.985\n",
      "epoch : 51,loss : 3.765\n",
      "epoch : 52,loss : 3.747\n",
      "epoch : 53,loss : 3.481\n",
      "epoch : 54,loss : 3.402\n",
      "epoch : 55,loss : 3.260\n",
      "epoch : 56,loss : 3.107\n",
      "epoch : 57,loss : 2.955\n",
      "epoch : 58,loss : 2.756\n",
      "epoch : 59,loss : 2.651\n",
      "epoch : 60,loss : 2.487\n",
      "epoch : 61,loss : 2.310\n",
      "epoch : 62,loss : 2.273\n",
      "epoch : 63,loss : 2.297\n",
      "epoch : 64,loss : 2.171\n",
      "epoch : 65,loss : 2.037\n",
      "epoch : 66,loss : 2.093\n",
      "epoch : 67,loss : 2.039\n",
      "epoch : 68,loss : 1.880\n",
      "epoch : 69,loss : 1.794\n",
      "epoch : 70,loss : 1.672\n",
      "epoch : 71,loss : 1.819\n",
      "epoch : 72,loss : 1.680\n",
      "epoch : 73,loss : 1.488\n",
      "epoch : 74,loss : 1.508\n",
      "epoch : 75,loss : 1.529\n",
      "epoch : 76,loss : 1.535\n",
      "epoch : 77,loss : 1.569\n",
      "epoch : 78,loss : 1.337\n",
      "epoch : 79,loss : 1.332\n",
      "epoch : 80,loss : 1.252\n",
      "epoch : 81,loss : 1.248\n",
      "epoch : 82,loss : 1.228\n",
      "epoch : 83,loss : 1.220\n",
      "epoch : 84,loss : 1.219\n",
      "epoch : 85,loss : 1.120\n",
      "epoch : 86,loss : 1.058\n",
      "epoch : 87,loss : 1.121\n",
      "epoch : 88,loss : 1.169\n",
      "epoch : 89,loss : 1.086\n",
      "epoch : 90,loss : 0.947\n",
      "epoch : 91,loss : 1.091\n",
      "epoch : 92,loss : 1.016\n",
      "epoch : 93,loss : 0.968\n",
      "epoch : 94,loss : 0.917\n",
      "epoch : 95,loss : 0.898\n",
      "epoch : 96,loss : 0.920\n",
      "epoch : 97,loss : 0.940\n",
      "epoch : 98,loss : 0.831\n",
      "epoch : 99,loss : 0.828\n",
      "Training avg loss : 19.285\n",
      "Test acc : 0.805 (1717/2133)\n",
      "Test loss : 0.154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model,data,batch_size,n_epoch):\n",
    "    model.train()\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(),lr = 0.1)\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        random.shuffle(data)\n",
    "        for i in range(0,len(data)-batch_size,batch_size):\n",
    "            in_data,labels = [],[]\n",
    "            for sentence, label in data[i:i+batch_size]:\n",
    "                index_vec = [w2i[w] for w in sentence]\n",
    "                pad_len = max(0,max_sentence_len - len(index_vec))\n",
    "                index_vec +=[0]*pad_len\n",
    "                index_vec = index_vec[:max_sentence_len]\n",
    "                in_data.append(index_vec)\n",
    "                labels.append(label)\n",
    "            sent_var = Variable(torch.LongTensor(in_data))\n",
    "            if use_cuda:\n",
    "                sent_var = sent_var.cuda()\n",
    "            target_var = Variable(torch.Tensor(labels).unsqueeze(1))\n",
    "            if use_cuda:\n",
    "                target_var = target_var.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(sent_var)\n",
    "            loss = F.binary_cross_entropy(probs,target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data[0]\n",
    "        \n",
    "        print('epoch : {:d},loss : {:.3f}'.format(epoch,epoch_loss))\n",
    "        losses.append(epoch_loss)\n",
    "    print('Training avg loss : {:.3f}'.format(sum(losses)/len(losses)))\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "def test(model,data, n_test,min_sentence_len):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for sentence,label in data[:n_test]:\n",
    "        if len(sentence) < min_sentence_len:\n",
    "            continue\n",
    "        index_vec = [w2i[w] for w in sentence]\n",
    "        sent_var = Variable(torch.LongTensor([index_vec]))\n",
    "        if use_cuda:\n",
    "            sent_var = sent_var.cuda()\n",
    "        out = model(sent_var)\n",
    "        score = out.data[0][0]\n",
    "        pred = 1 if score > .5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        loss += math.pow((label-score),2)\n",
    "    print('Test acc : {:.3f} ({:d}/{:d})'.format(correct/n_test,correct,n_test))\n",
    "    print('Test loss : {:.3f}'.format(loss/n_test))\n",
    "    \n",
    "out_ch = 100\n",
    "embd_size = 300\n",
    "batch_size = 50\n",
    "n_epoch = 100\n",
    "filter_size = [3,4,5]\n",
    "print('filter : ',filter_size)\n",
    "model = Net(vocab_size,embd_size,out_ch,filter_size,wv_matrix)\n",
    "model,losses = train(model,train_data,batch_size,n_epoch)\n",
    "test(model,test_data,len(test_data),max(filter_size))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
