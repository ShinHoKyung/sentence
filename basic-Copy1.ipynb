{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath,label):\n",
    "    data = []\n",
    "    with codecs.open(fpath,'r','utf-8',errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            data.append((l.split(' '),label))\n",
    "    return data\n",
    "pos = load_data('./dataset/rt-polarity.pos',1)\n",
    "neg = load_data('./dataset/rt-polarity.neg',0)\n",
    "data = pos+neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = max([len(sentence) for sentence, _ in data])\n",
    "\n",
    "vocab=[]\n",
    "\n",
    "for d, _ in data:\n",
    "    for w in d:\n",
    "        if w not in vocab:\n",
    "            vocab.append(w)\n",
    "#vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "i2w = {i:w for i,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_idx = (int)(len(data)*0.8)\n",
    "random.shuffle(data)\n",
    "train_data = data[:div_idx]\n",
    "test_data = data[div_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,vocab_size,embd_size,out_chs,filter_heights):\n",
    "        super(Net,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embd_size)\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1,out_chs,(fh,embd_size)) for fh in filter_heights])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filter_heights),1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "        x = [F.max_pool1d(i,i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        probs = F.sigmoid(x)\n",
    "        return probs\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter :  [3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 124.552\n",
      "epoch: 1, loss: 121.787\n",
      "epoch: 2, loss: 118.910\n",
      "epoch: 3, loss: 117.775\n",
      "epoch: 4, loss: 114.830\n",
      "epoch: 5, loss: 113.177\n",
      "epoch: 6, loss: 111.654\n",
      "epoch: 7, loss: 110.186\n",
      "epoch: 8, loss: 109.919\n",
      "epoch: 9, loss: 107.810\n",
      "epoch: 10, loss: 106.595\n",
      "epoch: 11, loss: 105.173\n",
      "epoch: 12, loss: 103.547\n",
      "epoch: 13, loss: 103.499\n",
      "epoch: 14, loss: 102.050\n",
      "epoch: 15, loss: 100.896\n",
      "epoch: 16, loss: 99.516\n",
      "epoch: 17, loss: 98.870\n",
      "epoch: 18, loss: 97.758\n",
      "epoch: 19, loss: 96.275\n",
      "epoch: 20, loss: 95.927\n",
      "epoch: 21, loss: 94.593\n",
      "epoch: 22, loss: 94.019\n",
      "epoch: 23, loss: 92.675\n",
      "epoch: 24, loss: 91.722\n",
      "epoch: 25, loss: 90.320\n",
      "epoch: 26, loss: 90.041\n",
      "epoch: 27, loss: 88.505\n",
      "epoch: 28, loss: 87.740\n",
      "epoch: 29, loss: 86.762\n",
      "epoch: 30, loss: 86.267\n",
      "epoch: 31, loss: 84.732\n",
      "epoch: 32, loss: 83.893\n",
      "epoch: 33, loss: 83.227\n",
      "epoch: 34, loss: 82.342\n",
      "epoch: 35, loss: 80.500\n",
      "epoch: 36, loss: 79.936\n",
      "epoch: 37, loss: 78.745\n",
      "epoch: 38, loss: 77.818\n",
      "epoch: 39, loss: 76.878\n",
      "epoch: 40, loss: 76.183\n",
      "epoch: 41, loss: 75.269\n",
      "epoch: 42, loss: 74.510\n",
      "epoch: 43, loss: 73.987\n",
      "epoch: 44, loss: 73.741\n",
      "epoch: 45, loss: 71.973\n",
      "epoch: 46, loss: 70.685\n",
      "epoch: 47, loss: 70.434\n",
      "epoch: 48, loss: 67.927\n",
      "epoch: 49, loss: 69.099\n",
      "epoch: 50, loss: 67.809\n",
      "epoch: 51, loss: 67.015\n",
      "epoch: 52, loss: 65.980\n",
      "epoch: 53, loss: 64.854\n",
      "epoch: 54, loss: 64.197\n",
      "epoch: 55, loss: 62.841\n",
      "epoch: 56, loss: 62.211\n",
      "epoch: 57, loss: 62.402\n",
      "epoch: 58, loss: 60.892\n",
      "epoch: 59, loss: 59.981\n",
      "epoch: 60, loss: 59.497\n",
      "epoch: 61, loss: 58.069\n",
      "epoch: 62, loss: 57.242\n",
      "epoch: 63, loss: 57.685\n",
      "epoch: 64, loss: 56.653\n",
      "epoch: 65, loss: 53.985\n",
      "epoch: 66, loss: 54.756\n",
      "epoch: 67, loss: 53.826\n",
      "epoch: 68, loss: 53.582\n",
      "epoch: 69, loss: 51.733\n",
      "epoch: 70, loss: 51.517\n",
      "epoch: 71, loss: 50.953\n",
      "epoch: 72, loss: 51.017\n",
      "epoch: 73, loss: 49.704\n",
      "epoch: 74, loss: 49.310\n",
      "epoch: 75, loss: 49.783\n",
      "epoch: 76, loss: 48.273\n",
      "epoch: 77, loss: 47.223\n",
      "epoch: 78, loss: 47.140\n",
      "epoch: 79, loss: 46.089\n",
      "epoch: 80, loss: 45.624\n",
      "epoch: 81, loss: 45.358\n",
      "epoch: 82, loss: 43.971\n",
      "epoch: 83, loss: 43.168\n",
      "epoch: 84, loss: 42.820\n",
      "epoch: 85, loss: 42.466\n",
      "epoch: 86, loss: 41.310\n",
      "epoch: 87, loss: 42.138\n",
      "epoch: 88, loss: 41.600\n",
      "epoch: 89, loss: 39.752\n",
      "epoch: 90, loss: 39.542\n",
      "epoch: 91, loss: 39.213\n",
      "epoch: 92, loss: 39.288\n",
      "epoch: 93, loss: 37.679\n",
      "epoch: 94, loss: 37.495\n",
      "epoch: 95, loss: 37.423\n",
      "epoch: 96, loss: 36.619\n",
      "epoch: 97, loss: 36.256\n",
      "epoch: 98, loss: 36.348\n",
      "epoch: 99, loss: 35.904\n",
      "Training avg loss: 71.054\n",
      "Test acc: 0.660 (1408/2133)\n",
      "Test loss: 0.200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, batch_size, n_epoch):\n",
    "    model.train() \n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(),lr = 0.01)\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        random.shuffle(data)\n",
    "        for i in range(0, len(data)-batch_size, batch_size): \n",
    "            in_data, labels = [], []\n",
    "            for sentence, label in data[i: i+batch_size]:\n",
    "                index_vec = [w2i[w] for w in sentence]\n",
    "                pad_len = max(0, max_sentence_len - len(index_vec))\n",
    "                index_vec += [0] * pad_len\n",
    "                index_vec = index_vec[:max_sentence_len] \n",
    "                in_data.append(index_vec)\n",
    "                labels.append(label)\n",
    "            sent_var = Variable(torch.LongTensor(in_data))\n",
    "            if use_cuda: sent_var = sent_var.cuda()\n",
    "\n",
    "            target_var = Variable(torch.Tensor(labels).unsqueeze(1))\n",
    "            if use_cuda: target_var = target_var.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(sent_var)\n",
    "            loss = F.binary_cross_entropy(probs, target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data[0]\n",
    "        print('epoch: {:d}, loss: {:.3f}'.format(epoch, epoch_loss))\n",
    "        losses.append(epoch_loss)\n",
    "    print('Training avg loss: {:.3f}'.format(sum(losses)/len(losses)))\n",
    "        \n",
    "    return model, losses\n",
    "\n",
    "def test(model, data, n_test, min_sentence_len):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for sentence, label in data[:n_test]:\n",
    "        if len(sentence) < min_sentence_len:  \n",
    "            continue\n",
    "        index_vec = [w2i[w] for w in sentence]\n",
    "        sent_var = Variable(torch.LongTensor([index_vec]))\n",
    "        if use_cuda: sent_var = sent_var.cuda()\n",
    "        out = model(sent_var)\n",
    "        score = out.data[0][0]\n",
    "        pred = 1 if score > .5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        loss += math.pow((label-score), 2)\n",
    "    print('Test acc: {:.3f} ({:d}/{:d})'.format(correct/n_test, correct, n_test))\n",
    "    print('Test loss: {:.3f}'.format(loss/n_test))\n",
    "    \n",
    "out_ch = 100\n",
    "embd_size = 300\n",
    "batch_size = 50\n",
    "n_epoch = 100\n",
    "filter_size = [3,4,5]\n",
    "print('filter : ',filter_size)\n",
    "model = Net(vocab_size,embd_size,out_ch,filter_size)\n",
    "model,losses = train(model,train_data,batch_size,n_epoch)\n",
    "test(model,test_data,len(test_data),max(filter_size))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
